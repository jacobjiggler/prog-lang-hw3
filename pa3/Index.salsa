
module pa3;

import java.util.HashMap;
import java.util.TreeMap;
import java.util.Iterator;
import java.util.Map;
import java.nio.charset.Charset;
import java.io.BufferedWriter;
import java.nio.file.Files;
import java.io.File;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;

behavior Index {
	
	int doc_num = 0;
	int worker_num = 0;

    void act(String[] argv) {
        // get number of documents and workers
        this.doc_num = Integer.parseInt(argv[2]);
        this.worker_num = Integer.parseInt(argv[3]);

		if (doc_num == 0 || worker_num == 0)
			return;
        // parse docs, get frequencies, and then compute dfs, all concurrently
        token all_docs = create_workers(argv[0]);


        // Create a treemap of idfs 
        token output = compute_idf(all_docs);

        // Grab a super long string of the terms and idfs values
        //token output = output_terms(treemap, all_docs);

        // save to text files
        //write_output(output, argv[1]);
    }

    // Create a new worker for each document
    Doc_Reader[] create_workers(String folder) {

    // Create the worker objects
		Doc_Reader[] workers = new Doc_Reader[this.worker_num];
		join {
			for (int i = 0; i < this.worker_num; ++i) {
				workers[i] = new Doc_Reader();
				workers[i] <- setDocLimit(doc_num/worker_num + 1); 
			}
		} @ output(token);
	
	//loop through both workers and docs distributing the docs between the workers
	
		int worker_itr = 0;
		join{
			for (int i = 0; i < doc_num; i++){
				if (worker_itr >= worker_num)
					worker_itr = 0;
				workers[worker_itr]<- parsedoc(Paths.get(folder, Integer.toString(i) + ".txt").toString());
				worker_itr++;
			}
		} @  output(token) @ currentContinuation;
	}
	//create a treemap of the amount of documents that a word shows up in
	void compute_idf(Object[] docs){
		String all_tfidf;
		TreeMap idfs = new TreeMap();	
		
	   for (int i = 0; i < docs.length; ++i) {
		HashMap tfs = (HashMap)docs[i];
		Iterator itr = tfs.keySet().iterator();
		while (itr.hasNext()) {
			String key = (String)itr.next();
			this.insert(idfs, key); 
		}
	}
	
	Iterator itr = idfs.entrySet().iterator();
	// Compute idf values from counts
	join {
		while (itr.hasNext()) {
			Map.Entry entry = (Map.Entry)itr.next();
			String key = (String)entry.getKey();
			int count = (int)entry.getValue();
			double idf = this.idf(count, doc_num);
			idfs.put(key, idf);
			//System.out.println(key + " " + count + " " + idf(count, doc_num));
			grab_all_listings(idf, key, docs);			
		} 
	} @ combine_array(token) @ currentContinuation;
	}
	
	String combine_array(Object[] string_array){
		StringBuilder loljava = new StringBuilder();//lolololololololololol What is this shit
		for (int i = 0; i < string_array.length; i++){
			loljava.append((String)string_array[i]);
		}
		return loljava.toString();
	}
	void grab_all_listings(double idf, String word, HashMap[] docs){
		join {
			for (int i = 0; i < doc_num; i++){
				Object tf = docs[i].get(word);
				double answer = 0;
				if (tf != null)
					answer = (double)tf;
					
				get_tfidf(answer, idf, i);
			}
		} @ combine_array(token) @ combine_strings(word, token) @ combine_strings(token, "\n") @ currentContinuation;
	}
	String get_tfidf(double tf, double idf, int doc_id){
        if (tf == 0) {
            return "";
        }

        return " " + doc_id + " " + String.format("%.4f", tf * idf);
	
	}
	void insert(TreeMap map, String key) {
		Object count = map.get(key);
		if (count == null) {
			map.put(key, new Integer(1));
		} else {
			map.put(key, new Integer((int)count) + 1);
		}
}

	
	//Why does ithe language require this? I'm not even using the output
	Object[] output(Object[] input){
		return input;
	}
	// Getting the idf values
	double idf(int value, int N) {
        return Math.log10(((double)(N + 2)) / ((double)(1 + value))); 
    }
}
